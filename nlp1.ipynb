{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "nlp1.ipynb",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**There are two main parts to Natural Language Processing**:\n",
        "\n",
        "- Data Preprocessing\n",
        "- Algorithm Development\n",
        "\n",
        "This the first step where we will started with NLP and understand the basic concepts and tools that are needed to work with text data. We shall be focusing mainly on the first and the most crucial part of Natural Language Processing – Text Preprocessing."
      ],
      "metadata": {
        "id": "V3VeH99QbFco"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset used for this step can be found here: https://www.kaggle.com/datasets/datatattle/email-classification-nlp"
      ],
      "metadata": {
        "id": "ssOGP092bFcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('SMS_test.csv',encoding='cp1252')\n",
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-01T12:04:01.965611Z",
          "iopub.execute_input": "2022-08-01T12:04:01.966078Z",
          "iopub.status.idle": "2022-08-01T12:04:02.007119Z",
          "shell.execute_reply.started": "2022-08-01T12:04:01.966036Z",
          "shell.execute_reply": "2022-08-01T12:04:02.005980Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "AcqvY299bFcq",
        "outputId": "d7bcb9b0-6127-4f96-e48a-285497c4963c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   S. No.                                       Message_body Label\n",
              "0       1  UpgrdCentre Orange customer, you may now claim...  Spam\n",
              "1       2  Loan for any purpose £500 - £75,000. Homeowner...  Spam\n",
              "2       3  Congrats! Nokia 3650 video camera phone is you...  Spam\n",
              "3       4  URGENT! Your Mobile number has been awarded wi...  Spam\n",
              "4       5  Someone has contacted our dating service and e...  Spam"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3775b3a4-bd5b-4b27-8143-523ba13f01af\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>S. No.</th>\n",
              "      <th>Message_body</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>UpgrdCentre Orange customer, you may now claim...</td>\n",
              "      <td>Spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Loan for any purpose £500 - £75,000. Homeowner...</td>\n",
              "      <td>Spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Congrats! Nokia 3650 video camera phone is you...</td>\n",
              "      <td>Spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>URGENT! Your Mobile number has been awarded wi...</td>\n",
              "      <td>Spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Someone has contacted our dating service and e...</td>\n",
              "      <td>Spam</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3775b3a4-bd5b-4b27-8143-523ba13f01af')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3775b3a4-bd5b-4b27-8143-523ba13f01af button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3775b3a4-bd5b-4b27-8143-523ba13f01af');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing For NLP\n",
        "\n",
        "Data Preprocessing is the most essential step for any Machine Learning model. How well the raw data has been cleaned and preprocessed plays a major role in the performance of the model. Likewise in the case of NLP, the very first step is Text Processing.\n",
        "\n",
        "The various preprocessing steps that are involved are :\n",
        "\n",
        "- Lower Casing\n",
        "- Tokenization\n",
        "- Punctuation Mark Removal\n",
        "- Stop Word Removal\n",
        "- Stemming\n",
        "- Lemmatization"
      ],
      "metadata": {
        "id": "MBTUHdN4bFcr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Pre-processing Using Lower Casing\n",
        "\n"
      ],
      "metadata": {
        "id": "4V_jx_JIbFcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = df.iloc[0]['Message_body']\n",
        "print(\"Before lower casing: \"+ sentence)\n",
        "sentence = sentence.lower()\n",
        "print(\"After lower casing: \"+ sentence)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-01T12:10:38.768330Z",
          "iopub.execute_input": "2022-08-01T12:10:38.774841Z",
          "iopub.status.idle": "2022-08-01T12:10:38.799358Z",
          "shell.execute_reply.started": "2022-08-01T12:10:38.774748Z",
          "shell.execute_reply": "2022-08-01T12:10:38.797609Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrpjxNsAbFcs",
        "outputId": "7fe0478e-7a05-48d5-8a55-934790acabb8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before lower casing: UpgrdCentre Orange customer, you may now claim your FREE CAMERA PHONE upgrade for your loyalty. Call now on 0207 153 9153. Offer ends 26th July. T&C's apply. Opt-out available\n",
            "After lower casing: upgrdcentre orange customer, you may now claim your free camera phone upgrade for your loyalty. call now on 0207 153 9153. offer ends 26th july. t&c's apply. opt-out available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understand Tokenization In Text Pre-processing\n",
        " \n",
        "**Tokenization** is the process of breaking up the paragraph into smaller units such as sentences or words. Each unit is then considered as an individual token.\n",
        "\n",
        "The fundamental principle of Tokenization is to try to understand the meaning of the text by analyzing the smaller units or tokens that constitute the paragraph.\n",
        "\n",
        "**NLTK** is the Natural Language Toolkit library in python that is used for Text Preprocessing."
      ],
      "metadata": {
        "id": "z0cqcxdObFcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-01T12:11:39.534876Z",
          "iopub.execute_input": "2022-08-01T12:11:39.535326Z",
          "iopub.status.idle": "2022-08-01T12:11:59.580728Z",
          "shell.execute_reply.started": "2022-08-01T12:11:39.535287Z",
          "shell.execute_reply": "2022-08-01T12:11:59.579480Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4qGkRVRbFct",
        "outputId": "dc5e67fd-b795-4e02-d93d-caf9bf8a488c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sentence Tokenize\n",
        "\n",
        "Now we shall take a paragraph as input and tokenize it into its constituting sentences. The result is a list stored in the variable ‘sentences’. It contains each sentence of the paragraph. The length of the list gives us the total number of sentences."
      ],
      "metadata": {
        "id": "5VVYzFFWbFct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph=df.iloc[14]['Message_body']\n",
        "print(\"Paragraph: \"+paragraph)\n",
        "# Tokenize Sentences\n",
        "sentences = nltk.sent_tokenize(paragraph.lower())\n",
        "print(\"Paragraph breakdown into sentences : \"+ str(sentences))\n",
        "print (\"No. of Sentences: \"+str(len(sentences)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-01T12:12:03.624560Z",
          "iopub.execute_input": "2022-08-01T12:12:03.625802Z",
          "iopub.status.idle": "2022-08-01T12:12:03.633721Z",
          "shell.execute_reply.started": "2022-08-01T12:12:03.625757Z",
          "shell.execute_reply": "2022-08-01T12:12:03.632512Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIzk1KsKbFcu",
        "outputId": "ea595d39-d389-4c42-873b-70d769fcf377"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraph: Ur cash-balance is currently 500 pounds - to maximize ur cash-in now send CASH to 86688 only 150p/msg. CC: 08708800282 HG/Suite342/2Lands Row/W1J6HL\n",
            "Paragraph breakdown into sentences : ['ur cash-balance is currently 500 pounds - to maximize ur cash-in now send cash to 86688 only 150p/msg.', 'cc: 08708800282 hg/suite342/2lands row/w1j6hl']\n",
            "No. of Sentences: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Word Tokenize\n",
        "\n",
        "Similarly, we can also tokenize the paragraph into words. The result is a list called ‘words’, containing each word of the paragraph. The length of the list gives us the total number of words present in our paragraph."
      ],
      "metadata": {
        "id": "IRkfTC74bFcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize Words\n",
        "paragraph=df.iloc[14]['Message_body']\n",
        "words = nltk.word_tokenize(paragraph.lower())\n",
        "print(words)\n",
        "print(\"No. of Words: \"+str(len(words)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-01T12:12:11.489175Z",
          "iopub.execute_input": "2022-08-01T12:12:11.490482Z",
          "iopub.status.idle": "2022-08-01T12:12:11.503754Z",
          "shell.execute_reply.started": "2022-08-01T12:12:11.490416Z",
          "shell.execute_reply": "2022-08-01T12:12:11.502223Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYgswxSFbFcv",
        "outputId": "8b78ecfc-bf13-4a6e-8c35-c4611861eb8e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ur', 'cash-balance', 'is', 'currently', '500', 'pounds', '-', 'to', 'maximize', 'ur', 'cash-in', 'now', 'send', 'cash', 'to', '86688', 'only', '150p/msg', '.', 'cc', ':', '08708800282', 'hg/suite342/2lands', 'row/w1j6hl']\n",
            "No. of Words: 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note: Here we observe that the punctuations have also been considered as separate words.**"
      ],
      "metadata": {
        "id": "kmL0AFBybFcv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Punctuation Mark Removal\n",
        "\n",
        "This brings us to the next step. We must now remove the punctuation marks from our list of words.\n",
        "\n",
        "We can remove all the punctuation marks from our list of words by excluding any alphanumeric element."
      ],
      "metadata": {
        "id": "q0h5o7rdbFcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_words= [word for word in words if word.isalnum()]\n",
        "print(new_words)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-01T12:12:14.207810Z",
          "iopub.execute_input": "2022-08-01T12:12:14.208219Z",
          "iopub.status.idle": "2022-08-01T12:12:14.215786Z",
          "shell.execute_reply.started": "2022-08-01T12:12:14.208186Z",
          "shell.execute_reply": "2022-08-01T12:12:14.214313Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZYH2wFzbFcv",
        "outputId": "0129c59f-c7ec-478b-e3f8-86df32af86e9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ur', 'is', 'currently', '500', 'pounds', 'to', 'maximize', 'ur', 'now', 'send', 'cash', 'to', '86688', 'only', 'cc', '08708800282']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stop Word Removal\n",
        "\n",
        "**Stop words** are a collection of words that occur frequently in any language but do not add much meaning to the sentences. These are common words that are part of the grammar of any language. Every language has its own set of stop words. For example some of the English stop words are “the”, “he”, “him”, “his”, “her”, “herself” etc."
      ],
      "metadata": {
        "id": "iQwGuFvUbFcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-01T12:12:16.862223Z",
          "iopub.execute_input": "2022-08-01T12:12:16.862687Z",
          "iopub.status.idle": "2022-08-01T12:12:36.904860Z",
          "shell.execute_reply.started": "2022-08-01T12:12:16.862648Z",
          "shell.execute_reply": "2022-08-01T12:12:36.903413Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQNsjsOHbFcw",
        "outputId": "5879c48b-3037-4f62-cefe-ea8a63f95f2f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-01T12:13:05.086243Z",
          "iopub.execute_input": "2022-08-01T12:13:05.087364Z",
          "iopub.status.idle": "2022-08-01T12:13:05.111824Z",
          "shell.execute_reply.started": "2022-08-01T12:13:05.087304Z",
          "shell.execute_reply": "2022-08-01T12:13:05.110615Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtJxfkrXbFcw",
        "outputId": "97bd5653-7aba-4ac6-edb4-edf45648b1f7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are all the English stop words.\n",
        "\n",
        "You can also get the stop words of other languages by simply changing the parameter. Have some fun and try passing “Spanish” or “French” as the parameter!\n",
        "\n",
        "Since these stop words do not add much value to the overall meaning of the sentence, we can easily remove these words from our text data. This helps in **dimensionality reduction** by eliminating unnecessary information."
      ],
      "metadata": {
        "id": "H9PqYWi0bFcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WordSet = []\n",
        "for word in new_words:\n",
        "   if word not in set(stopwords.words(\"english\")):\n",
        "      WordSet.append(word)\n",
        "print(WordSet)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-01T12:13:14.670768Z",
          "iopub.execute_input": "2022-08-01T12:13:14.671383Z",
          "iopub.status.idle": "2022-08-01T12:13:14.702999Z",
          "shell.execute_reply.started": "2022-08-01T12:13:14.671330Z",
          "shell.execute_reply": "2022-08-01T12:13:14.701747Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAIHIGe1bFcw",
        "outputId": "d41128b3-0710-4750-bb19-d8b4c47fcfb2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ur', 'currently', '500', 'pounds', 'maximize', 'ur', 'send', 'cash', '86688', 'cc', '08708800282']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(WordSet))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-01T12:13:17.523857Z",
          "iopub.execute_input": "2022-08-01T12:13:17.524282Z",
          "iopub.status.idle": "2022-08-01T12:13:17.530636Z",
          "shell.execute_reply.started": "2022-08-01T12:13:17.524245Z",
          "shell.execute_reply": "2022-08-01T12:13:17.529456Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcfPBCwkbFcx",
        "outputId": "7ed94c85-f239-4607-eb99-d2f842c64434"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming\n",
        "\n",
        "**Stemming** is the process of reduction of a word into its root or stem word. The word affixes are removed leaving behind only the root form or lemma.\n",
        "\n",
        "For example: The words “connecting”, “connect”, “connection”, “connects” are all reduced to the root form “connect”. The words “studying”, “studies”, “study” are all reduced to “studi”.\n"
      ],
      "metadata": {
        "id": "f2VfjEkfbFcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-01T12:14:13.758861Z",
          "iopub.execute_input": "2022-08-01T12:14:13.759316Z",
          "iopub.status.idle": "2022-08-01T12:14:13.764306Z",
          "shell.execute_reply.started": "2022-08-01T12:14:13.759272Z",
          "shell.execute_reply": "2022-08-01T12:14:13.763389Z"
        },
        "trusted": true,
        "id": "E1acFrcnbFcx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph=\"Linguistics is the scientific study of language. It encompasses the analysis of every aspect of language, as well as the methods for studying and modeling them. The traditional areas of linguistic analysis include phonetics, phonology, morphology, syntax, semantics, and pragmatics.\"\n",
        "words = nltk.word_tokenize(paragraph.lower())\n",
        "new_words= [word for word in words if word.isalnum()]\n",
        "WordSet = []\n",
        "for word in new_words:\n",
        "   if word not in set(stopwords.words(\"english\")):\n",
        "      WordSet.append(word)\n",
        "\n",
        "print(\"Before Stemming: \"+ str(WordSet))\n",
        "\n",
        "WordSetStem = []\n",
        "for word in WordSet:\n",
        "   WordSetStem.append(ps.stem(word))\n",
        "print(\"After Stemming: \"+ str(WordSetStem))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-01T12:14:18.164952Z",
          "iopub.execute_input": "2022-08-01T12:14:18.165527Z",
          "iopub.status.idle": "2022-08-01T12:14:18.193251Z",
          "shell.execute_reply.started": "2022-08-01T12:14:18.165462Z",
          "shell.execute_reply": "2022-08-01T12:14:18.192041Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhQJdU6pbFcx",
        "outputId": "e84a3ddf-5080-4745-b58e-cc83d3d0ff21"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Stemming: ['linguistics', 'scientific', 'study', 'language', 'encompasses', 'analysis', 'every', 'aspect', 'language', 'well', 'methods', 'studying', 'modeling', 'traditional', 'areas', 'linguistic', 'analysis', 'include', 'phonetics', 'phonology', 'morphology', 'syntax', 'semantics', 'pragmatics']\n",
            "After Stemming: ['linguist', 'scientif', 'studi', 'languag', 'encompass', 'analysi', 'everi', 'aspect', 'languag', 'well', 'method', 'studi', 'model', 'tradit', 'area', 'linguist', 'analysi', 'includ', 'phonet', 'phonolog', 'morpholog', 'syntax', 'semant', 'pragmat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Note**: The word list obtained after performing stemming does not always contain words that are a part of the English vocabulary. In our example, words such as “scientif“, “studi“, “everi” are not proper words, i.e. they do not make sense to us."
      ],
      "metadata": {
        "id": "VVEuEDICbFcy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization\n",
        "We have just seen, how we can reduce the words to their root words using Stemming.\n",
        "\n",
        "However, Stemming does not always result in words that are part of the language vocabulary. It often results in words that have no meaning to the users. In order to overcome this drawback, we shall use the concept of Lemmatization."
      ],
      "metadata": {
        "id": "85tZ2rKLbFcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lm= WordNetLemmatizer()\n",
        "nltk.download('wordnet')\n",
        "import nltk\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-01T12:15:12.617259Z",
          "iopub.execute_input": "2022-08-01T12:15:12.617803Z",
          "iopub.status.idle": "2022-08-01T12:15:52.695714Z",
          "shell.execute_reply.started": "2022-08-01T12:15:12.617762Z",
          "shell.execute_reply": "2022-08-01T12:15:52.694417Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qZWlR40bFcy",
        "outputId": "43a54db0-771c-485c-de64-1292e5f25ef0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WordSetLem = []\n",
        "for word in WordSet:\n",
        "   WordSetLem.append(lm.lemmatize(word))\n",
        "print(WordSetLem)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-01T12:15:52.823926Z",
          "iopub.status.idle": "2022-08-01T12:15:52.824934Z",
          "shell.execute_reply.started": "2022-08-01T12:15:52.824621Z",
          "shell.execute_reply": "2022-08-01T12:15:52.824652Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kv0yUBGWbFcy",
        "outputId": "479e60b5-907e-42d2-ddd3-013dc0c8a4d9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['linguistics', 'scientific', 'study', 'language', 'encompasses', 'analysis', 'every', 'aspect', 'language', 'well', 'method', 'studying', 'modeling', 'traditional', 'area', 'linguistic', 'analysis', 'include', 'phonetics', 'phonology', 'morphology', 'syntax', 'semantics', 'pragmatic']\n"
          ]
        }
      ]
    }
  ]
}